{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhZmt5HL7jJWInb5vT3ZD5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sfansaria/Automated-Speech-Recognition-System-for-Spoken-digits-Using-Deep-Learning/blob/main/ASR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iDU2Xh3u4LD"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "main.py for running the main training code\n",
        "CNN_model.py has all the model classes\n",
        "plot_results.py plots the graphs by loading model history\n",
        "model_load.py loads a given model and plots confusion matrix\n",
        "'''\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from CNN_model import CNNSpeech\n",
        "from CNN_model import ResNetSpeech\n",
        "from CNN_model import CNNSpeechRegularised\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import pickle\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "\n",
        "# building dataset from image directory\n",
        "def dataset(train,val):\n",
        "    train_set = keras.utils.image_dataset_from_directory(\n",
        "        directory=train,\n",
        "        labels=\"inferred\",\n",
        "        color_mode=\"grayscale\",\n",
        "        label_mode=\"categorical\",\n",
        "        batch_size=128,\n",
        "        image_size=(98, 50))\n",
        "\n",
        "    val_set = keras.utils.image_dataset_from_directory(\n",
        "        directory=val,\n",
        "        labels=\"inferred\",\n",
        "        color_mode=\"grayscale\",\n",
        "        label_mode=\"categorical\",\n",
        "        batch_size=128,\n",
        "        image_size=(98, 50))\n",
        "    return train_set, val_set\n",
        "\n",
        "\n",
        "#main training function\n",
        "def train(train_set,val_set, model, batch_size, epochs):\n",
        "\n",
        "    # compile the keras model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # fit the keras model on the dataset\n",
        "    history = model.fit(train_set,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs,\n",
        "                        verbose=True, validation_data=val_set)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# training function for learning rate decay experiments in two stages\n",
        "def train_learning_rate_exp(train_set,val_set, model, batch_size, epochs):\n",
        "\n",
        "    # compile the keras model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # fit the keras model on the dataset\n",
        "    history = model.fit(train_set,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs,\n",
        "                        verbose=True, validation_data=val_set)\n",
        "    train_acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    K.set_value(model.optimizer.learning_rate, 0.0001)\n",
        "    history = model.fit(train_set,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs,\n",
        "                        verbose=True, validation_data=val_set)\n",
        "\n",
        "    train_acc = train_acc + history.history['accuracy']\n",
        "    val_acc = val_acc + history.history['val_accuracy']\n",
        "\n",
        "    plt.plot(train_acc)\n",
        "    plt.plot(val_acc)\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "# elementary plotting function\n",
        "def plot(history):\n",
        "    print(history.history.keys())\n",
        "    # history for accuracy\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model evaluation\n",
        "def evaluate_model(model, val_set):\n",
        "    # Generate generalization metrics\n",
        "    #score = model.evaluate(val_set, verbose=1)\n",
        "    #print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
        "    _, accuracy = model.evaluate(val_set)\n",
        "    print('Accuracy: %.2f' % (accuracy*100))\n",
        "\n",
        "\n",
        "def save_model(model):\n",
        "    model.save_weights(\"saved_models/model_weights\", save_format='tf')\n",
        "\n",
        "\n",
        "def show_model(model,filename):\n",
        "    plot_model(model, to_file=filename, show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# save model training history\n",
        "def save_history(history,filename):\n",
        "    outbuffer = open(filename, 'wb')\n",
        "    pickle.dump(history,outbuffer)\n",
        "    outbuffer.close()\n",
        "\n",
        "# load model history given a path\n",
        "def load_history(filename):\n",
        "    inbuffer = open(filename,'rb')\n",
        "    history = pickle.load(inbuffer)\n",
        "    return history\n",
        "\n",
        "\n",
        "def save_model(model,filename):\n",
        "    model.save(filename)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_data = \"/home/saba/PycharmProjects/saba/speechImageData/TrainingData\"\n",
        "    val_data = \"/home/saba/PycharmProjects/saba/speechImageData/ValidationData\"\n",
        "    train_set, val_set = dataset(train_data,val_data)\n",
        "    model = CNNSpeech()\n",
        "    # model = CNNSpeechRegularised()\n",
        "    # model = ResNetSpeech()\n",
        "    batch_size = 128\n",
        "    epochs = 100\n",
        "    model, history = train(train_set,val_set,model,batch_size,epochs)\n",
        "    # model, history = train_learning_rate_exp(train_set, val_set, model, batch_size, epochs)\n",
        "    print(model.optimizer.learning_rate)\n",
        "    plot(history)\n",
        "    evaluate_model(model, val_set)\n",
        "    save_history(history, \"history/test.hist\")\n",
        "    save_model(model, \"model/test\")\n",
        "    print(model.summary())\n",
        "\n"
      ],
      "metadata": {
        "id": "nIrTN5icvLBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "model class file\n",
        "'''\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import BatchNormalization, MaxPool2D\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "from keras.models import load_model\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "class CNNSpeech(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    num_of_blocks = 4\n",
        "    self.input_cnn = Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\")\n",
        "    self.cnn_block = Sequential()\n",
        "    # dynamic creation of convolution blocks\n",
        "    for layers in range(num_of_blocks):\n",
        "        self.cnn_block.add(Conv2D(512, (3, 3), strides=(1, 1), activation=\"relu\"))\n",
        "        self.cnn_block.add(BatchNormalization())\n",
        "        self.cnn_block.add(MaxPool2D((2,2),padding='same'))\n",
        "    self.flatten = Flatten()\n",
        "    self.dense1 = Dense(512, activation=\"relu\")\n",
        "    self.dense2 = Dense(12, activation=\"softmax\")\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # print(inputs.shape)\n",
        "    x = self.input_cnn(inputs)\n",
        "    # print(x.shape)\n",
        "    x = self.cnn_block(x)\n",
        "    # print(x.shape)\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense1(x)\n",
        "    x = self.dense2(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "636SvnHfvR81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNSpeechRegularised(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    num_of_blocks = 4\n",
        "    self.input_cnn = Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\")\n",
        "    self.cnn_block = Sequential()\n",
        "    # dynamic creation of convolution blocks\n",
        "    for layers in range(num_of_blocks):\n",
        "        self.cnn_block.add(Conv2D(512, (3, 3), strides=(1, 1), activation=\"relu\"))\n",
        "        self.cnn_block.add(BatchNormalization())\n",
        "        self.cnn_block.add(MaxPool2D((2,2),padding='same'))\n",
        "    self.flatten = Flatten()\n",
        "    self.dense1 = Dense(512, activation=\"relu\",\n",
        "    kernel_regularizer=keras.regularizers.l2(l=0.01))\n",
        "    self.dense2 = Dense(128, activation=\"relu\")\n",
        "    self.dense3 = Dense(17, activation=\"softmax\")\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # print(inputs.shape)\n",
        "    x = self.input_cnn(inputs)\n",
        "    # print(x.shape)\n",
        "    x = self.cnn_block(x)\n",
        "    # print(x.shape)\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense1(x)\n",
        "    x = self.dense2(x)\n",
        "    x = self.dense3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "gywEIEU-vZWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetSpeech(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    num_of_blocks = 2\n",
        "    self.input_cnn = Conv2D(32, (3, 3), strides=(1, 1), activation=\"relu\")\n",
        "    self.cnn_block1 = Sequential()\n",
        "    # dynamic creation of convolution blocks\n",
        "    for layers in range(num_of_blocks):\n",
        "        self.cnn_block1.add(Conv2D(128, (3, 3), strides=(1, 1), activation=\"relu\"))\n",
        "        self.cnn_block1.add(BatchNormalization())\n",
        "        self.cnn_block1.add(MaxPool2D((2,2),padding='same'))\n",
        "\n",
        "    self.cnn_block2 = Sequential()\n",
        "    for layers in range(num_of_blocks):\n",
        "      self.cnn_block2.add(Conv2D(128, (1, 1), strides=(1, 1), activation=\"relu\"))\n",
        "      self.cnn_block2.add(BatchNormalization())\n",
        "      # self.cnn_block2.add(MaxPool2D((2, 2), padding='same'))\n",
        "\n",
        "    self.cnn_block3 = Sequential()\n",
        "    for layers in range(num_of_blocks):\n",
        "      self.cnn_block3.add(Conv2D(256, (3, 3), strides=(1, 1), activation=\"relu\"))\n",
        "      self.cnn_block3.add(BatchNormalization())\n",
        "      self.cnn_block3.add(MaxPool2D((2, 2), padding='same'))\n",
        "\n",
        "    self.cnn_block4 = Sequential()\n",
        "    for layers in range(num_of_blocks):\n",
        "      self.cnn_block4.add(Conv2D(256, (1, 1), strides=(1, 1), activation=\"relu\"))\n",
        "      self.cnn_block4.add(BatchNormalization())\n",
        "      # self.cnn_block2.add(MaxPool2D((2, 2), padding='same'))\n",
        "\n",
        "    self.flatten = Flatten()\n",
        "    self.dense1 = Dense(256, activation=\"relu\",\n",
        "    kernel_regularizer=keras.regularizers.l2(l=0.01))\n",
        "    self.dense2 = Dense(128, activation=\"sigmoid\")\n",
        "    self.dense3 = Dense(12, activation=\"sigmoid\")\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # print(inputs.shape)\n",
        "    x = self.input_cnn(inputs)\n",
        "    # print(x.shape)\n",
        "    x = self.cnn_block1(x)\n",
        "    # print(x.shape)\n",
        "    x_ = x\n",
        "    x = self.cnn_block2(x)\n",
        "    # print(x.shape)\n",
        "    x = tf.keras.layers.Add()([x, x_])\n",
        "    # print(x.shape)\n",
        "    x = self.cnn_block3(x)\n",
        "    # print(x.shape)\n",
        "    x_ = x\n",
        "    x = self.cnn_block4(x)\n",
        "    # print(x.shape)\n",
        "    x = tf.keras.layers.Add()([x, x_])\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense1(x)\n",
        "    x = self.dense2(x)\n",
        "    x = self.dense3(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kE3fv8rqvedo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This program does different types of plotting based on model comparisons\n",
        "'''\n",
        "\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_three_comparison(history1,history2,history3):\n",
        "    print(history1.history.keys())\n",
        "    # summarize history for test accuracy\n",
        "    plt.plot(history1.history['val_accuracy'])\n",
        "    plt.plot(history2.history['val_accuracy'])\n",
        "    plt.plot(history3.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['4 CNNBlocks each 128 filter', '4 CNNBlocks each 256 filter',\n",
        "    '4 CNNBlocks each 512 filter'], loc='lower right')\n",
        "    plt.grid(True)\n",
        "    plt.minorticks_on()\n",
        "    plt.show()\n",
        "    # summarize history for loss\n",
        "    print(history1.history.keys())\n",
        "    # summarize history for test accuracy\n",
        "    plt.plot(history1.history['val_loss'])\n",
        "    plt.plot(history2.history['val_loss'])\n",
        "    plt.plot(history3.history['val_loss'])\n",
        "    ax = plt.gca()\n",
        "    ax.set_ylim([0, 1])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['4 CNNBlocks each 128 filter', '4 CNNBlocks each 256 filter',\n",
        "    '4 CNNBlocks each 512 filter'], loc='upper right')\n",
        "    plt.grid(True)\n",
        "    plt.minorticks_on()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_two_comparison(history1,history2):\n",
        "    print(history1.history.keys())\n",
        "    # summarize history for test accuracy\n",
        "    plt.plot(history1.history['val_accuracy'])\n",
        "    plt.plot(history2.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['4 CNNBlocks each 512 filter with regularisation',\n",
        "    '4 CNNBlocks each 512 filter'], loc='lower right')\n",
        "    plt.grid(True)\n",
        "    plt.minorticks_on()\n",
        "    plt.show()\n",
        "    # summarize history for loss\n",
        "    print(history1.history.keys())\n",
        "    # summarize history for test accuracy\n",
        "    plt.plot(history1.history['val_loss'])\n",
        "    plt.plot(history2.history['val_loss'])\n",
        "    ax = plt.gca()\n",
        "    ax.set_ylim([0, 1])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['4 CNNBlocks each 512 filter with regularisation',\n",
        "    '4 CNNBlocks each 512 filter'], loc='upper right')\n",
        "    plt.grid(True)\n",
        "    plt.minorticks_on()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot(history):\n",
        "    print(history.history.keys())\n",
        "    # summarize history for accuracy\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_CNN', 'test_CNN'], loc='upper left')\n",
        "    plt.minorticks_on()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    ax = plt.gca()\n",
        "    ax.set_ylim([0, 2])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_CNN', 'test_CNN'], loc='upper left')\n",
        "    plt.minorticks_on()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def load_history(filename):\n",
        "    inbuffer = open(filename,'rb')\n",
        "    history = pickle.load(inbuffer)\n",
        "    return history\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    history1 = load_history(\"/home/saba/PycharmProjects/saba/\n",
        "    history/model_4_block_128.hist\")\n",
        "    history2 = load_history(\"/home/saba/PycharmProjects/saba/\n",
        "    history/model_4_block_256.hist\")\n",
        "    history3 = load_history(\"/home/saba/PycharmProjects/saba/\n",
        "    history/model_4_block_512.hist\")\n",
        "    plot_three_comparison(history1, history2, history3)\n",
        "    history1 = load_history(\"/home/saba/PycharmProjects/saba/\n",
        "    history/model_4_block_512_regularised.hist\")\n",
        "    history2 = load_history(\"/home/saba/PycharmProjects/saba/\n",
        "    history/model_4_block_512.hist\")\n",
        "    plot_two_comparison(history1,history2)\n",
        "    history1 = load_history(\"/home/saba/PycharmProjects/saba/\n",
        "    history/model_4_512_extendeddata.hist\")\n",
        "    plot(history1)\n",
        "\n"
      ],
      "metadata": {
        "id": "KYHFWl8uvlOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "load model and print confusion matrix.\n",
        "'''\n",
        "from CNN_model import CNNSpeech\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "def dataset(train,val):\n",
        "    train_set = keras.utils.image_dataset_from_directory(\n",
        "        directory=train,\n",
        "        labels=\"inferred\",\n",
        "        color_mode=\"grayscale\",\n",
        "        label_mode=\"categorical\",\n",
        "        batch_size=128,\n",
        "        image_size=(98, 50))\n",
        "\n",
        "    val_set = keras.utils.image_dataset_from_directory(\n",
        "        directory=val,\n",
        "        labels=\"inferred\",\n",
        "        color_mode=\"grayscale\",\n",
        "        label_mode=\"categorical\",\n",
        "        batch_size=128,\n",
        "        image_size=(98, 50))\n",
        "    return train_set, val_set\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_data = \"/home/saba/PycharmProjects/saba/speechImageData/TrainingData\"\n",
        "    val_data = \"/home/saba/PycharmProjects/saba/speechImageData/ValidationData\"\n",
        "    train_set, val_set = dataset(train_data,val_data)\n",
        "    classes = [\"yes\",\"no\",\"up\",\"down\",\"left\",\"right\",\"on\",\"off\",\"stop\",\"go\",\n",
        "    \"background\", \"unknown\"]\n",
        "    true_label = []\n",
        "    pred_label = []\n",
        "    model= keras.models.load_model(\"/home/saba/PycharmProjects/\n",
        "    saba/model/model_resnet\")\n",
        "    model.compile()\n",
        "    for data, label in val_set:\n",
        "        y_pred = model.predict(data)\n",
        "        y_pred = np.argmax(y_pred,axis=1)\n",
        "        label = label.numpy()\n",
        "        label = np.where(label==1)[1]\n",
        "        true_label.append(label)\n",
        "        pred_label.append(y_pred)\n",
        "\n",
        "    y_pred = np.concatenate(pred_label,axis=0)\n",
        "    y_true = np.concatenate(true_label,axis=0)\n",
        "    print(y_pred.shape)\n",
        "    print(y_true.shape)\n",
        "    conf_obj = confusion_matrix(y_true, y_pred)\n",
        "    display = ConfusionMatrixDisplay(confusion_matrix=conf_obj,\n",
        "    display_labels=classes)\n",
        "    display.plot()\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ONN910DjvwD9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}